{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "id": "T4DzRFAdneG5",
    "outputId": "3ea24532-fec9-4188-bdcf-18499efb5a01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kEtJeE2rTvaU"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jX3o7WXA6NAf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "import keras\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from keras.layers import Layer\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Flatten, Dropout, Reshape, Concatenate\n",
    "from keras.layers import LSTM, ConvLSTM2D, TimeDistributed, Conv1D, CuDNNGRU \n",
    "from keras.layers import MaxPooling1D, BatchNormalization\n",
    "from keras import initializers, regularizers, constraints, optimizers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CPuAP1zOMd44"
   },
   "outputs": [],
   "source": [
    "class AttentionWithoutContext(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(AttentionWithoutContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if(len(input_shape) != 3):\n",
    "            print(len(input_shape))\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "71NFG-V9Mh23"
   },
   "outputs": [],
   "source": [
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "    \n",
    "\n",
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "        Attention operation, with a context/query vector, for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "        \"Hierarchical Attention Networks for Document Classification\"\n",
    "        by using a context vector to assist the attention\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(AttentionWithContext())\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True,\n",
    "                 return_attention=False, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        # ait = K.dot(uit, self.u)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        if self.return_attention:\n",
    "            return [result, a]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], input_shape[-1]),\n",
    "                    (input_shape[0], input_shape[1])]\n",
    "        else:\n",
    "            return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "boKo3ihSMliP"
   },
   "outputs": [],
   "source": [
    "#gru model with attention\n",
    "def get_attn_lstm_model(n_timesteps, n_features, n_outputs):\n",
    "    inputs = Input(shape=(n_timesteps,n_features,))\n",
    "    gru = CuDNNGRU(196,return_sequences=True) (inputs)\n",
    "    gru = BatchNormalization() (gru)\n",
    "    x1 = AttentionWithoutContext(n_timesteps)  (gru)\n",
    "    x2 = AttentionWithContext() (gru)\n",
    "    x = Concatenate() ([x1,x2])\n",
    "    x = BatchNormalization() (x)\n",
    "    x = Dense(512, activation='relu') (x)\n",
    "    x = Dropout(0.2) (x)\n",
    "    x = Dense(128, activation='relu') (x)\n",
    "    predictions = Dense(n_outputs, activation='softmax') (x)\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XVKkM6P7MqBF"
   },
   "outputs": [],
   "source": [
    "#2 layer gru model with attention\n",
    "def get_attn_lstm_model2(n_timesteps, n_features, n_outputs):\n",
    "    inputs = Input(shape=(n_timesteps,n_features,))\n",
    "    gru1 = CuDNNGRU(200,return_sequences=True) (inputs)\n",
    "    gru1 = BatchNormalization() (gru1)\n",
    "    xg1 = AttentionWithoutContext(n_timesteps)  (gru1)\n",
    "    xg2 = AttentionWithContext() (gru1)\n",
    "    gru2 = CuDNNGRU(200,return_sequences=True) (gru1)\n",
    "    gru2 = BatchNormalization() (gru2)\n",
    "    xg3 = AttentionWithoutContext(n_timesteps)  (gru2)\n",
    "    xg4 = AttentionWithContext() (gru2)\n",
    "    x = Concatenate() ([xg1,xg2,xg3,xg4])\n",
    "    x = BatchNormalization() (x)\n",
    "    x = Dense(512, activation='relu') (x)\n",
    "    x = Dropout(0.25) (x)\n",
    "    x = Dense(128, activation='relu') (x)\n",
    "    x = Dropout(0.1) (x)\n",
    "    predictions = Dense(n_outputs, activation='softmax') (x)\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "66T1xT4CMuQN"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def getxy(df,sub,act,n):\n",
    "  \n",
    "  b=df[act]\n",
    "  a=df.drop(columns=[sub,act])\n",
    "  \n",
    "  \n",
    "  x=a.values\n",
    "  y=b.values\n",
    "  \n",
    "  a=x.reshape(x.shape[0]//n,n,-1)\n",
    "  b=y.reshape(y.shape[0]//n,n,-1)\n",
    "  \n",
    "  return a,b\n",
    "\n",
    "\n",
    "def to_cata(col):\n",
    "  data = col.values\n",
    "  data = array(data)\n",
    "  encoded = to_categorical(data)\n",
    "  \n",
    "  return encoded\n",
    "  \n",
    "  \n",
    "  \n",
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "\n",
    "\n",
    "def getc(y_test,y_pred):\n",
    "  matrix = confusion_matrix(y_test, y_pred)\n",
    "  return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 888
    },
    "colab_type": "code",
    "id": "S1DdJTc_NaLF",
    "outputId": "b2c28a09-4c82-4067-d419-b57d044a108e"
   },
   "outputs": [],
   "source": [
    "verbose, epochs, batch_size = 0, 10, 128\n",
    "earlyStopping = EarlyStopping(monitor='val_acc', patience=20,verbose=0, mode='max')\n",
    "mcp_save = ModelCheckpoint('test_3_best.hdf5', save_best_only=True, monitor='val_acc', mode='max')\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=6, verbose=0, min_delta=1e-4, mode='min')\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score,accuracy_score, confusion_matrix, f1_score\n",
    "\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df=pd.read_csv('drive/My Drive/total.csv')\n",
    "\n",
    "a=df['subject'].unique()\n",
    "N=50\n",
    "#n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "#model = get_attn_lstm_model2(n_timesteps, n_features, n_outputs)\n",
    "\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "avac=[]\n",
    "avrec=[]\n",
    "avprec=[]\n",
    "avf=[]\n",
    "\n",
    "for i in range(6):\n",
    "  train=df[(df.subject != a[i])]\n",
    "  test=df.loc[df['subject'].isin([a[i]])]\n",
    "  \n",
    "  train_x,train_y=getxy(train,'subject','activityID',N)\n",
    "  \n",
    "  test_x,test_y=getxy(test,'subject','activityID',N)\n",
    "  train_y = to_categorical(train_y[:,0,0])\n",
    "  testgt = test_y[:,0,0]\n",
    "  test_y = to_categorical(test_y[:,0,0])\n",
    "  \n",
    "  n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "  model = get_attn_lstm_model2(n_timesteps, n_features, n_outputs)\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  \n",
    "  \n",
    "  \n",
    "  model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size,\n",
    "              verbose=verbose, validation_data=(test_x, test_y),\n",
    "             callbacks=[earlyStopping, mcp_save,reduce_lr_loss])\n",
    "\n",
    "#predict\n",
    "  test_model = get_attn_lstm_model2(n_timesteps,n_features,n_outputs)\n",
    "  test_model.load_weights('test_3_best.hdf5')\n",
    "#   modelpred = test_model.predict(test_x,batch_size=batch_size, verbose=0)\n",
    "#   pred = prediction(modelpred)\n",
    "#   print(test_x.shape, testgt.shape, pred.shape)\n",
    "\n",
    "# evaluate model\n",
    "  \n",
    "  test_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc',f1_m,precision_m, recall_m])\n",
    "  loss, accuracy, f1_score, precision, recall= test_model.evaluate(test_x, test_y, batch_size=batch_size, verbose=0)\n",
    "  print(\"folding\",i+1)\n",
    "  print(\"loss\",loss)\n",
    "  print('accuracy: ' + str(accuracy))\n",
    "  print('precision: ' + str(precision))\n",
    "  print('recall: ' + str(recall))\n",
    "  print('F score: ' + str(f1_score))\n",
    "  print(\" \")\n",
    "  avac.append(accuracy)\n",
    "  avrec.append(recall)\n",
    "  avprec.append(precision)\n",
    "  avf.append(f1_score)\n",
    "  \n",
    "\n",
    "print(\"************average result********\")\n",
    "print(\" \")\n",
    "print('accuracy: ' + str(mean(avac)))\n",
    "print('precision: ' + str(mean(avprec)))\n",
    "print('recall: ' + str(mean(avrec)))\n",
    "print('F score: ' + str(mean(avf)))\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WekCS80FqoNQ"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "new.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
